{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def LinearQuantizeOut(x, k, alpha):                # only quantize >0 values (relu must be preceded)\n",
    "    L = 2.**k - 1\n",
    "    xdiv = x.div(alpha)\n",
    "    xc = xdiv.clamp(min=0., max=1.)\n",
    "    xq = xc.mul(L).round()\n",
    "    xmul = xq.div(L).mul(alpha)\n",
    "    return xmul\n",
    "\n",
    "def LinearQuantizeW(x, k, max_val, min_val):       # asymetric quant\n",
    "    delta = max_val - min_val\n",
    "    L= 2 ** k - 1\n",
    "    stepSize = delta / L\n",
    "    index = torch.clamp(torch.round((x-min_val) / delta * L), 0, L)\n",
    "    return index, index * stepSize + min_val, stepSize\n",
    "\n",
    "\n",
    "# Ideally, same to inputQ * weightQ if ADC=32bits\n",
    "def MAC(inputQ, weightQ, abits, wbits, adcbits, output_size, subArray):\n",
    "    assert wbits >= 2\n",
    "    outputreal = F.linear(inputQ, weightQ, None)\n",
    "    # print(f'real: {F.linear(inputQ, weightQ, None)}')\n",
    "    outputShiftIN = torch.zeros_like(outputreal)\n",
    "    for z in range(abits):\n",
    "        inputB = torch.fmod(inputQ, 2)              # 12,10,14 = [0,0,0] / [0,1,1] / [1,0,1] / [1,1,1]\n",
    "        inputQ = torch.round((inputQ-inputB)/2)     # 12,10,14 = [6,5,7] / [3,2,3] / [1,1,1] / [0,0,0]\n",
    "        weightQb = weightQ\n",
    "        outputShiftW = torch.zeros_like(outputreal)\n",
    "        for k in range (wbits):\n",
    "            weightB = torch.fmod(weightQb, 2)\n",
    "            weightQb = torch.round((weightQb-weightB)/2)\n",
    "            outputPartial = F.linear(inputB, weightB, None)\n",
    "            # Add ADC quanization effects here !!!\n",
    "            outputADC = LinearQuantizeOut(outputPartial, adcbits, subArray)\n",
    "            # shift per w bit sequence\n",
    "            outputShiftW = outputShiftW + outputADC * (2 ** k)\n",
    "        # shift per input bit sequence\n",
    "        outputShiftIN = outputShiftIN + outputShiftW * (2 ** z)\n",
    "    # since inputQ [0, 15] when k=4, rescale output by divide 16\n",
    "    # output = output + inputS * (outputIN * wS + outputIND * w.min())     # suppose I=[0~15], W=[-8~7] -> I*W = I[0~15]*W[0~15] + I[0~15]*W_constant[-8] (which is w.min())\n",
    "    if output_size != outputShiftIN.size():\n",
    "        outputShiftIN = outputShiftIN.transpose(1,2).reshape(output_size)\n",
    "    return outputShiftIN\n",
    "\n",
    "# Ideally, same to inputQ * weightQ if ADC=32bits\n",
    "def CAM(inputQ, weightQ, abits, wbits, adcbits, output_size, subArray):\n",
    "    # print(f'real: {F.linear(inputQ, weightQi, None)}')\n",
    "    outputreal = F.linear(inputQ, weightQ, None)\n",
    "    outputShiftIN = torch.zeros_like(outputreal)\n",
    "    for z in range(abits):\n",
    "        inputB = torch.fmod(inputQ, 2)              # 12,10,14 = [0,0,0] / [0,1,1] / [1,0,1] / [1,1,1]\n",
    "        inputQ = torch.round((inputQ-inputB)/2)     # 12,10,14 = [6,5,7] / [3,2,3] / [1,1,1] / [0,0,0]\n",
    "        uniqs = torch.unique(weightQ)\n",
    "        outputShiftW = torch.zeros_like(outputreal)\n",
    "        # outputShiftW = F.linear(inputB, weightQ, None)  # approximately ~16.5s\n",
    "        for un in uniqs:    # approximately ~28s\n",
    "            maskCAM = (weightQ == un).float()\n",
    "            outML = F.linear(inputB, maskCAM, None)\n",
    "            # outMLADC = LinearQuantizeOut(outML, adcbits, subArray)\n",
    "            outMLADC = outML # (original is of course better)\n",
    "            outputShiftW = outputShiftW + outMLADC * un\n",
    "        outputShiftIN = outputShiftIN + outputShiftW * (2 ** z)\n",
    "        \n",
    "    if output_size != outputShiftIN.size():\n",
    "        outputShiftIN = outputShiftIN.transpose(1,2).reshape(output_size)\n",
    "    return outputShiftIN\n",
    "\n",
    "# Ideally, same to inputQ * weightQ if ADC=32bits\n",
    "def ZP_MAC(inputQ, weightQ, abits, adcbits, output_size, subArray, zero_point_opt):\n",
    "    outputreal = F.linear(inputQ, weightQ, None)\n",
    "    outputDummyShift = torch.zeros_like(outputreal)\n",
    "    if zero_point_opt:\n",
    "        outputDummyShift = F.linear(inputQ, weightQ, None)\n",
    "    else:\n",
    "        for z in range(abits):\n",
    "            inputB = torch.fmod(inputQ, 2)              \n",
    "            inputQ = torch.round((inputQ-inputB)/2)     \n",
    "            outputDummy = F.linear(inputB, weightQ, None)\n",
    "            outputDummyADC = LinearQuantizeOut(outputDummy, adcbits, subArray)\n",
    "            outputDummyShift = outputDummyShift + outputDummyADC * (2 ** z)\n",
    "    # since inputQ [0, 15] when k=4, rescale output by divide 16\n",
    "    # output = output + inputS * (outputIN * wS + outputIND * w.min())     # suppose I=[0~15], W=[-8~7] -> I*W = I[0~15]*W[0~15] + I[0~15]*W_constant[-8] (which is w.min())\n",
    "    if output_size != outputDummyShift.size():\n",
    "        outputDummyShift = outputDummyShift.transpose(1,2).reshape(output_size)\n",
    "    return outputDummyShift\n",
    "    \n",
    "    \n",
    "# default: vanilla\n",
    "# inference = 0: real (FP32)\n",
    "# inference = 1: Activation/Weight Quantize\n",
    "# inference = 2: Activation/Weight/Output Quantize\n",
    "# inference = 3: PIM (Activation/Weight/Output Quantize)\n",
    "# inference = -1: PIM-mimic (where consider dummy)\n",
    "class QConv2dCAM(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=False, bitActivation=8,bitWeight=[4,4],\n",
    "                 inference=0,subArray=128,bitADC=5,zero_point_opt=False):\n",
    "        super(QConv2dCAM, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
    "        assert isinstance(bitWeight, list)\n",
    "        self.bitWeight = bitWeight\n",
    "        self.bitWeightMSB, self.bitWeightLSB = bitWeight\n",
    "        self.bitActivation = bitActivation\n",
    "        self.inference = inference\n",
    "        self.subArray = subArray\n",
    "        self.bitADC = bitADC\n",
    "        self.zero_point_opt = zero_point_opt\n",
    "        \n",
    "    def extra_repr(self):\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "        s += ', ibits={bitActivation}, wbits={bitWeight}, inference={inference}, subArray={subArray}, ADCbits={bitADC}'\n",
    "        return s.format(**self.__dict__)\n",
    "        \n",
    "    # convert input -> # [N, OH * OW, IC * KH * KW]\n",
    "    def input_2d_mapping(self, input):\n",
    "        fold_param = dict(kernel_size=self.kernel_size, dilation=self.dilation, padding=self.padding, stride=self.stride)\n",
    "        unfold_module = nn.Unfold(**fold_param)\n",
    "        unfold_out = unfold_module(input)\n",
    "        return unfold_out.transpose(1,2)\n",
    "    \n",
    "    # convert: weight -> # [IC * KH * KW, OC]\n",
    "    def weight_2d_mapping(self, weight):\n",
    "        return weight.reshape(weight.shape[0], -1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # make input & weight to 2D tensors\n",
    "        input2D = self.input_2d_mapping(input)                                                           \n",
    "        weight2D = self.weight_2d_mapping(self.weight)                              \n",
    "\n",
    "        inputQ, inputQS, inputS = LinearQuantizeW(input2D, self.bitActivation, input2D.max(), input2D.min())\n",
    "        weightQ, weightQS, weightS = LinearQuantizeW(weight2D, sum(self.bitWeight), weight2D.max(), weight2D.min())\n",
    "        \n",
    "        outputreal = F.conv2d(input, self.weight, self.bias, self.stride, self.padding)\n",
    "        weightQM = weightQ // (2.**self.bitWeightLSB)\n",
    "        if self.bitWeightMSB == 0:                              # if MSB=0bit, then use weightQ\n",
    "            weightQL = weightQ\n",
    "        else:\n",
    "            weightQL = weightQ % (2.**self.bitWeightLSB)\n",
    "        \n",
    "        numSubArray = int(weight2D.shape[1]/self.subArray)\n",
    "        if self.inference == 3:\n",
    "            if numSubArray == 0:\n",
    "                outputM = CAM(inputQ, weightQM, self.bitActivation, self.bitWeightMSB, self.bitADC, outputreal.size(), weight2D.shape[1])\n",
    "                outputL = MAC(inputQ, weightQL, self.bitActivation, self.bitWeightLSB, self.bitADC, outputreal.size(), weight2D.shape[1])\n",
    "                outputDL = ZP_MAC(inputQ, torch.ones_like(weightQ), self.bitActivation, self.bitADC, outputreal.size(), weight2D.shape[1], self.zero_point_opt)\n",
    "                \n",
    "                outputP = (outputM * (2.**self.bitWeightLSB) + outputL) * weightS\n",
    "                outputD = outputDL * weight2D.min()\n",
    "                out = inputS * (outputP + outputD)\n",
    "            else:\n",
    "                numSubRow = [self.subArray] * numSubArray + ([] if weight2D.shape[1] % self.subArray == 0 else [weight2D.shape[1] % self.subArray])\n",
    "                out = torch.zeros_like(outputreal)\n",
    "                for s, rowArray in enumerate(numSubRow):\n",
    "                    mask = torch.zeros_like(weight2D)\n",
    "                    mask[:,(s*self.subArray):(s+1)*self.subArray] = 1\n",
    "                    outputM = CAM(inputQ, weightQM*mask, self.bitActivation, self.bitWeightMSB, self.bitADC, outputreal.size(), rowArray)\n",
    "                    outputL = MAC(inputQ, weightQL*mask, self.bitActivation, self.bitWeightLSB, self.bitADC, outputreal.size(), rowArray)\n",
    "                    outputDL = ZP_MAC(inputQ, torch.ones_like(weightQ)*mask, self.bitActivation, self.bitADC, outputreal.size(), rowArray, self.zero_point_opt)\n",
    "                    \n",
    "                    outputP = (outputM * (2.**self.bitWeightLSB) + outputL) * weightS\n",
    "                    outputD = outputDL * weight2D.min()\n",
    "                    out = out + inputS * (outputP + outputD)\n",
    "            if self.bias is not None:\n",
    "                out = out + self.bias\n",
    "        elif self.inference == 2:\n",
    "            output = F.linear(inputQS, weightQS, self.bias)\n",
    "            _, out, _ = LinearQuantizeW(output, self.bitADC, output.max(), output.min())\n",
    "        elif self.inference == 1:\n",
    "            out = F.linear(inputQS, weightQS, self.bias)\n",
    "        elif self.inference == -1:\n",
    "            outputM = F.linear(inputQ, weightQM, None).transpose(1,2).reshape(outputreal.size())\n",
    "            outputL = F.linear(inputQ, weightQL, None).transpose(1,2).reshape(outputreal.size())\n",
    "            outputDL = F.linear(inputQ, torch.ones_like(weightQ), None).transpose(1,2).reshape(outputreal.size())\n",
    "            \n",
    "            outputP = (outputM * (2.**self.bitWeightLSB) + outputL) * weightS\n",
    "            outputD = outputDL * weight2D.min()\n",
    "            out = inputS * (outputP + outputD)\n",
    "            if self.bias is not None:\n",
    "                out = out + self.bias\n",
    "        else:\n",
    "            out = outputreal\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subarray=original ====================\n",
      "tensor([-0.3429, -0.3891, -0.4199, -0.3860, -0.3704, -0.4122, -0.4087, -0.4121,\n",
      "        -0.3671, -0.3403], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "========================================\n",
      "subarray=quant MSB ----------\n",
      "tensor([67.0770, 70.0010, 70.0984, 69.7911, 69.7089, 69.3103, 69.5355, 69.6085,\n",
      "        69.7911, 68.3610], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "subarray=quant LSB ----------\n",
      "tensor([59.9326, 64.0829, 64.3324, 64.3020, 64.6793, 64.3507, 64.5728, 64.7767,\n",
      "        65.5130, 63.6630], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "subarray=quant zero_point ----------\n",
      "tensor([-1164.2705, -1219.4513, -1224.0117, -1215.8030, -1213.5228, -1210.7866,\n",
      "        -1214.4349, -1215.8030, -1215.3469, -1188.4407], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([-0.3453, -0.3925, -0.4230, -0.3868, -0.3719, -0.4160, -0.4140, -0.4140,\n",
      "        -0.3683, -0.3442], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "MSE Error: 0.0\n",
      "========================================\n",
      "subarray=qconv2dcam inference: 0 ----------\n",
      "tensor([-0.3429, -0.3891, -0.4199, -0.3860, -0.3704, -0.4122, -0.4087, -0.4121,\n",
      "        -0.3671, -0.3403], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "abits= 8\n",
    "wbits= 8\n",
    "inference= 3\n",
    "subarray= 100_000\n",
    "adcbits= 6\n",
    "zero_point_opt = True\n",
    "LSB_bits = 4\n",
    "\n",
    "x = torch.load(\"/app/base/base_input.pth\")[0][:10]\n",
    "w = torch.load(\"/app/base/base_weight.pth\")\n",
    "oc, ic, k, _ = w.size()\n",
    "s, p, d, g = 1, 0, 1, 1\n",
    "\n",
    "# test(x, w, abits, wbits, LSB_bits)\n",
    "print(f'subarray=original', '=='*10)\n",
    "L = nn.Conv2d(ic, oc, k, bias=False)\n",
    "L.weight.data = w\n",
    "origout = L(x)\n",
    "outputconvsize = origout.size()\n",
    "print(origout.flatten()[:10])\n",
    "print('=='*20)\n",
    "\n",
    "qx, qxs, xs = LinearQuantizeW(x, abits, x.max(), x.min())\n",
    "# print('qx: ', torch.unique(qx), torch.unique(qxs, return_counts=True))\n",
    "qw, qws, ws = LinearQuantizeW(w, wbits, w.max(), w.min())\n",
    "# print('qw: ', torch.unique(qw), torch.unique(qws, return_counts=True))\n",
    "L = QConv2dCAM(ic, oc, k, s, p, d, g, False, abits, [wbits-LSB_bits, LSB_bits], inference, subarray, adcbits, zero_point_opt)\n",
    "# 2D mapping\n",
    "iqx = L.input_2d_mapping(qx)\n",
    "iqw = L.weight_2d_mapping(qw)\n",
    "\n",
    "def linear(x, w, outputconvsize):\n",
    "    return F.linear(x, w, None).transpose(1,2).reshape(outputconvsize)\n",
    "\n",
    "print(f'subarray=quant MSB', '-'*10)\n",
    "msb_iqw = iqw // (2.**LSB_bits)\n",
    "msb_out = linear(iqx, msb_iqw, outputconvsize)\n",
    "msb_out_scale = msb_out * ws\n",
    "print(msb_out_scale.flatten()[:10])\n",
    "\n",
    "print(f'subarray=quant LSB', '-'*10)\n",
    "lsb_iqw = iqw % (2.**LSB_bits)\n",
    "lsb_out = linear(iqx, lsb_iqw, outputconvsize)\n",
    "lsb_out_scale = lsb_out * ws\n",
    "print(lsb_out_scale.flatten()[:10])\n",
    "\n",
    "print(f'subarray=quant zero_point', '-'*10)\n",
    "lsb_dum_out = linear(iqx, torch.ones_like(iqw), outputconvsize) * w.min()\n",
    "print(lsb_dum_out.flatten()[:10])\n",
    "\n",
    "out = xs * (msb_out_scale * (2.**LSB_bits) + lsb_out_scale + lsb_dum_out) \n",
    "print(out.flatten()[:10])\n",
    "print(f'MSE Error: {round(abs(torch.sum((out - origout)**2).item() / origout.nelement()), 2)}')\n",
    "print('=='*20)\n",
    "\n",
    "print(f'subarray=qconv2dcam inference: 0', '-'*10)\n",
    "inference = 0\n",
    "layer = QConv2dCAM(ic, oc, k, 1, 0, 1, 1, False, abits, [wbits-LSB_bits,LSB_bits], inference, 100_000, adcbits)\n",
    "layer.weight.data = w\n",
    "out = layer(x)\n",
    "print(out.flatten()[:10])\n",
    "print('=='*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subarray=qconv2dcam inference: 3 ----------\n",
      "tensor([-0.3429, -0.3891, -0.4199, -0.3860, -0.3704, -0.4122, -0.4087, -0.4121,\n",
      "        -0.3671, -0.3403], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "MSE Error: 0.0\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "print(f'subarray=qconv2dcam inference: 3', '-'*10)\n",
    "L = QConv2dCAM(ic, oc, k, s, p, d, g, False, abits, [wbits-LSB_bits, LSB_bits], inference, subarray, adcbits, zero_point_opt)\n",
    "L.weight.data = w\n",
    "out = L(x)\n",
    "print(out.flatten()[:10])\n",
    "print(f'MSE Error: {round(abs(torch.sum((out - origout)**2).item() / origout.nelement()), 2)}')\n",
    "print('=='*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
