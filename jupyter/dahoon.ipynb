{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "\n",
    "assert torch.cuda.is_available(), \\\n",
    "\"The current runtime does not have CUDA support.\" \\\n",
    "\"Please go to menu bar (Runtime - Change runtime type) and select GPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fab89ba86f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CifarResNet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jsw/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_resnet20\", pretrained=True) # pretrained cifar10 model load\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "  model: nn.Module,\n",
    "  dataloader: DataLoader,\n",
    "  criterion: nn.Module,\n",
    "  optimizer: Optimizer,\n",
    "  callbacks = None\n",
    ") -> None:\n",
    "  model.train()\n",
    "  model.cuda()\n",
    "\n",
    "  for inputs, targets in tqdm(dataloader, desc='train', leave=False):\n",
    "    # Move the data from CPU to GPU\n",
    "    inputs = inputs.cuda()\n",
    "    targets = targets.cuda()\n",
    "\n",
    "    # Reset the gradients (from the last iteration)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward inference\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Backward propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weight using optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    if callbacks is not None:\n",
    "        for callback in callbacks:\n",
    "            callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def evaluate(\n",
    "  model: nn.Module,\n",
    "  dataloader: DataLoader,\n",
    "  extra_preprocess = None\n",
    ") -> float:\n",
    "  model.eval()\n",
    "  model.cuda()\n",
    "  num_samples = 0\n",
    "  num_correct = 0\n",
    "\n",
    "  for inputs, targets in tqdm(dataloader, desc=\"eval\", leave=False):\n",
    "    # Move the data from CPU to GPU\n",
    "    inputs = inputs.cuda()\n",
    "    if extra_preprocess is not None:\n",
    "        for preprocess in extra_preprocess:\n",
    "            inputs = preprocess(inputs)\n",
    "\n",
    "    targets = targets.cuda()\n",
    "\n",
    "    # Inference\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Convert logits to class indices\n",
    "    outputs = outputs.argmax(dim=1)\n",
    "\n",
    "    # Update metrics\n",
    "    num_samples += targets.size(0)\n",
    "    num_correct += (outputs == targets).sum()\n",
    "\n",
    "  return (num_correct / num_samples * 100).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "transforms = {\n",
    "    \"train\": Compose([\n",
    "        RandomCrop(image_size, padding=4),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ]),\n",
    "    \"test\": Compose([ToTensor(), Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "}\n",
    "dataset = {}\n",
    "for split in [\"train\", \"test\"]:\n",
    "  dataset[split] = CIFAR10(\n",
    "    root=\"/home/jsw/data/cifar10\",\n",
    "    train=(split == \"train\"),\n",
    "    download=False,\n",
    "    transform=transforms[split],\n",
    "  )\n",
    "dataloader = {}\n",
    "for split in ['train', 'test']:\n",
    "  dataloader[split] = DataLoader(\n",
    "    dataset[split],\n",
    "    batch_size=512,\n",
    "    shuffle=(split == 'train'),\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp32 model has accuracy=92.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "fp32_model_accuracy = evaluate(model, dataloader['test'])\n",
    "print(f\"fp32 model has accuracy={fp32_model_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's First Evaluate the Accuracy and Model Size of the FP32 Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class _quantize_func(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, step_size, half_lvls):\n",
    "        # ctx is a context object that can be used to stash information\n",
    "        # for backward computation\n",
    "        ctx.step_size = step_size\n",
    "        ctx.half_lvls = half_lvls\n",
    "        output = F.hardtanh(input,\n",
    "                            min_val=-ctx.half_lvls * ctx.step_size.item(),\n",
    "                            max_val=ctx.half_lvls * ctx.step_size.item())\n",
    "\n",
    "        output = torch.round(output / ctx.step_size)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = grad_output.clone() / ctx.step_size\n",
    "\n",
    "        return grad_input, None, None\n",
    "\n",
    "\n",
    "quantize = _quantize_func.apply\n",
    "\n",
    "\n",
    "class QuantConv2d(nn.Conv2d):\n",
    "    N_bits = 8\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride=1,\n",
    "                 padding=0,\n",
    "                 dilation=1,\n",
    "                 groups=1,\n",
    "                 bias=True):\n",
    "        super(QuantConv2d, self).__init__(in_channels,\n",
    "                                          out_channels,\n",
    "                                          kernel_size,\n",
    "                                          stride=stride,\n",
    "                                          padding=padding,\n",
    "                                          dilation=dilation,\n",
    "                                          groups=groups,\n",
    "                                          bias=bias)\n",
    "        self.full_lvls = 2**self.N_bits\n",
    "        self.half_lvls = (self.full_lvls - 2) / 2\n",
    "        # Initialize the step size\n",
    "        self.step_size = nn.Parameter(torch.Tensor([1]), requires_grad=True)\n",
    "        self.__reset_stepsize__()\n",
    "        # flag to enable the inference with quantized weight or self.weight\n",
    "        self.inf_with_weight = False  # disabled by default\n",
    "\n",
    "        # create a vector to identify the weight to each bit\n",
    "        self.b_w = nn.Parameter(2**torch.arange(start=self.N_bits - 1,\n",
    "                                                end=-1,\n",
    "                                                step=-1).unsqueeze(-1).float(),\n",
    "                                requires_grad=False)\n",
    "\n",
    "        self.b_w[0] = -self.b_w[0]  #in-place change MSB to negative\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.inf_with_weight:\n",
    "            return F.conv2d(input, self.weight * self.step_size, self.bias,\n",
    "                            self.stride, self.padding, self.dilation,\n",
    "                            self.groups)\n",
    "        else:\n",
    "            self.__reset_stepsize__()\n",
    "            weight_quan = quantize(self.weight, self.step_size,\n",
    "                                   self.half_lvls) * self.step_size\n",
    "            input_step_size = input.max() / (self.full_lvls -1) # unsigned input\n",
    "            input_quan = quantize(input, input_step_size, (self.full_lvls -1)) * input_step_size\n",
    "            \n",
    "            #input_quan = input\n",
    "            return F.conv2d(input_quan, weight_quan, self.bias, self.stride,\n",
    "                            self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def __reset_stepsize__(self):\n",
    "        with torch.no_grad():\n",
    "            self.step_size.data = self.weight.abs().max() / self.half_lvls\n",
    "\n",
    "    def __reset_weight__(self):\n",
    "        '''\n",
    "        This function will reconstruct the weight stored in self.weight.\n",
    "        Replacing the original floating-point with the quantized fix-point\n",
    "        weight representation.\n",
    "        '''\n",
    "        # replace the weight with the quantized version\n",
    "        with torch.no_grad():\n",
    "            self.weight.data = quantize(self.weight, self.step_size,\n",
    "                                        self.half_lvls)\n",
    "        # enable the flag, thus now computation does not invovle weight quantization\n",
    "        self.inf_with_weight = True\n",
    "    \n",
    "    def __reset_half_lvls__(self):\n",
    "        '''\n",
    "        recunstruct half_lvls\n",
    "        '''\n",
    "        self.full_lvls = 2**self.N_bits\n",
    "        self.half_lvls = (self.full_lvls - 2) / 2\n",
    "\n",
    "class QuantLinear(nn.Linear):\n",
    "    N_bits = 8\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(QuantLinear, self).__init__(in_features, out_features, bias=bias)\n",
    "\n",
    "        self.full_lvls = 2**self.N_bits\n",
    "        self.half_lvls = (self.full_lvls - 2) / 2\n",
    "        # Initialize the step size\n",
    "        self.step_size = nn.Parameter(torch.Tensor([1]), requires_grad=True)\n",
    "        self.__reset_stepsize__()\n",
    "        # flag to enable the inference with quantized weight or self.weight\n",
    "        self.inf_with_weight = False  # disabled by default\n",
    "\n",
    "        # create a vector to identify the weight to each bit\n",
    "        self.b_w = nn.Parameter(2**torch.arange(start=self.N_bits - 1,\n",
    "                                                end=-1,\n",
    "                                                step=-1).unsqueeze(-1).float(),\n",
    "                                requires_grad=False)\n",
    "\n",
    "        self.b_w[0] = -self.b_w[0]  #in-place reverse\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.inf_with_weight:\n",
    "            return F.linear(input, self.weight * self.step_size, self.bias)\n",
    "        else:\n",
    "            self.__reset_stepsize__()\n",
    "            weight_quan = quantize(self.weight, self.step_size,\n",
    "                                   self.half_lvls) * self.step_size\n",
    "            input_step_size = input.max() / (self.full_lvls -1) # unsigned input\n",
    "            input_quan = quantize(input, input_step_size, (self.full_lvls -1)) * input_step_size\n",
    "            \n",
    "            #input_quan = input\n",
    "            return F.linear(input_quan, weight_quan, self.bias)\n",
    "\n",
    "    def __reset_stepsize__(self):\n",
    "        with torch.no_grad():\n",
    "            self.step_size.data = self.weight.abs().max() / self.half_lvls\n",
    "\n",
    "    def __reset_weight__(self):\n",
    "        '''\n",
    "        This function will reconstruct the weight stored in self.weight.\n",
    "        Replacing the orginal floating-point with the quantized fix-point\n",
    "        weight representation.\n",
    "        '''\n",
    "        # replace the weight with the quantized version\n",
    "        with torch.no_grad():\n",
    "            self.weight.data = quantize(self.weight, self.step_size,\n",
    "                                        self.half_lvls)\n",
    "        # enable the flag, thus now computation does not invovle weight quantization\n",
    "        self.inf_with_weight = True\n",
    "\n",
    "    def __reset_half_lvls__(self):\n",
    "        '''\n",
    "        recunstruct half_lvls\n",
    "        '''\n",
    "        self.full_lvls = 2**self.N_bits\n",
    "        self.half_lvls = (self.full_lvls - 2) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hook:\n",
    "    def __init__(self, module):\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.input = input\n",
    "        self.weight = module.weight\n",
    "        self.output = output\n",
    "\n",
    "        self.approx_weight = quantize(self.weight, module.step_size, module.half_lvls) * module.step_size\n",
    "        self.input_step_size = self.input[0].max() / (module.full_lvls -1) # unsigned input\n",
    "        self.approx_input = quantize(self.input[0], self.input_step_size, module.full_lvls -1) * self.input_step_size\n",
    "        self.approx_output = F.conv2d(self.approx_input, self.approx_weight, module.bias, module.stride,\n",
    "                            module.padding, module.dilation, module.groups)\n",
    "        \n",
    "    def close(self):\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 32, 32])\n",
      "torch.Size([64, 3, 3, 3])\n",
      "torch.Size([4, 64, 32, 32])\n",
      "torch.Size([64, 3, 3, 3])\n",
      "torch.Size([4, 3, 32, 32])\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "testConv = QuantConv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "inputs = torch.randn(4, 3, 32, 32)\n",
    "hook_list = []\n",
    "for name, module in testConv.named_modules():\n",
    "    if isinstance(module, QuantConv2d):\n",
    "        hook_list.append(Hook(module))\n",
    "\n",
    "testConv(inputs)\n",
    "for hook in hook_list:\n",
    "    print(hook.input[0].shape)\n",
    "    print(hook.weight.shape)\n",
    "    print(hook.output.shape)\n",
    "    print(hook.approx_weight.shape)\n",
    "    print(hook.approx_input.shape)\n",
    "    print(hook.approx_output - hook.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CifarResNet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "\n",
    "def set_deep_attr(obj, attrs, value):\n",
    "    for attr in attrs.split(\".\")[:-1]:\n",
    "        obj = getattr(obj, attr)\n",
    "    setattr(obj, attrs.split(\".\")[-1], value)\n",
    "\n",
    "\n",
    "def change_model(model):\n",
    "    copy_model = copy.deepcopy(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d) and 'downsample' not in name:\n",
    "            set_deep_attr(copy_model, name, QuantConv2d(module.in_channels, module.out_channels, module.kernel_size, module.stride, module.padding, module.dilation, module.groups, module.bias))\n",
    "            for p_name, p in module.named_parameters():\n",
    "                set_deep_attr(copy_model, name + '.' + p_name, p)\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            set_deep_attr(copy_model, name, QuantLinear(module.in_features, module.out_features, True if module.bias is not None else False))\n",
    "            for p_name, p in module.named_parameters():\n",
    "                set_deep_attr(copy_model, name + '.' + p_name, p)\n",
    "    return copy_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CifarResNet(\n",
      "  (conv1): QuantConv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): QuantLinear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "c_model = change_model(model)\n",
    "print(c_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int8 ptq model has accuracy=92.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "ptq_int8_model_accuracy = evaluate(c_model, dataloader['test'])\n",
    "print(f\"int8 ptq model has accuracy={ptq_int8_model_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int6 ptq model has accuracy=91.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "QuantConv2d.N_bits = 6\n",
    "c_model = change_model(model)\n",
    "ptq_int6_model_accuracy = evaluate(c_model, dataloader['test'])\n",
    "print(f\"int6 ptq model has accuracy={ptq_int6_model_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantized_Hook:\n",
    "    def __init__(self, name, module):\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "        self.name = name\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.module = module\n",
    "        self.input = input\n",
    "        self.weight = module.weight\n",
    "        self.output = output\n",
    "        self.quan_weight = quantize(self.weight, module.step_size, module.half_lvls)\n",
    "        self.input_step_size = self.input[0].max() / (module.full_lvls -1) # unsigned input\n",
    "        self.weight_step_size = module.step_size\n",
    "        self.quan_input = quantize(self.input[0], self.input_step_size, module.full_lvls -1)\n",
    "        self.quan_output = F.conv2d(self.quan_input, self.quan_weight, module.bias, module.stride,\n",
    "                            module.padding, module.dilation, module.groups)\n",
    "        \n",
    "    def close(self):\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[[[ 8.9407e-08, -7.4506e-09,  2.9802e-08,  2.9802e-08,  0.0000e+00,\n",
      "            0.0000e+00, -1.4901e-08,  2.9802e-08],\n",
      "          [ 0.0000e+00,  1.4901e-07,  3.1665e-08,  0.0000e+00,  1.1921e-07,\n",
      "           -5.2154e-08,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00, -1.4901e-08, -8.9407e-08,  0.0000e+00,  8.9407e-08,\n",
      "            2.9802e-08, -5.9605e-08,  5.9605e-08],\n",
      "          [ 0.0000e+00, -5.9605e-08,  0.0000e+00,  2.9802e-08,  5.9605e-08,\n",
      "            5.9605e-08,  1.1921e-07,  5.9605e-08],\n",
      "          [ 2.9802e-08, -5.9605e-08,  0.0000e+00,  5.9605e-08, -5.9605e-08,\n",
      "            0.0000e+00, -1.7881e-07,  0.0000e+00],\n",
      "          [-3.7253e-09,  8.9407e-08, -4.8429e-08,  3.7253e-09, -7.4506e-09,\n",
      "            5.9605e-08, -5.9605e-08,  4.4703e-08],\n",
      "          [-1.4901e-08,  0.0000e+00, -9.3132e-10, -1.4901e-08,  0.0000e+00,\n",
      "           -1.1921e-07,  0.0000e+00,  0.0000e+00],\n",
      "          [ 2.2352e-08, -1.1921e-07, -7.4506e-09,  1.4901e-08,  5.9605e-08,\n",
      "            5.9605e-08, -5.9605e-08,  0.0000e+00]],\n",
      "\n",
      "         [[ 1.4901e-08,  2.9802e-08, -8.3819e-09,  1.4901e-08, -1.4901e-08,\n",
      "            0.0000e+00,  5.9605e-08,  0.0000e+00],\n",
      "          [ 8.9407e-08,  1.1921e-07,  7.4506e-08, -2.9802e-08,  2.9802e-08,\n",
      "            0.0000e+00, -1.4901e-08,  7.4506e-09],\n",
      "          [ 2.9802e-08, -2.9802e-08, -2.9802e-08,  2.9802e-08,  0.0000e+00,\n",
      "            4.4703e-08, -1.4901e-08, -5.9605e-08],\n",
      "          [ 0.0000e+00,  8.9407e-08, -1.1921e-07, -5.9605e-08,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00, -1.1921e-07,  1.1921e-07,  5.9605e-08, -2.9802e-08,\n",
      "           -1.1921e-07, -5.9605e-08,  4.6566e-09],\n",
      "          [ 0.0000e+00,  8.9407e-08,  0.0000e+00, -4.4703e-08,  0.0000e+00,\n",
      "            1.4901e-08, -5.9605e-08,  3.7253e-09],\n",
      "          [-2.9802e-08, -2.9802e-08,  0.0000e+00,  0.0000e+00,  2.9802e-08,\n",
      "            0.0000e+00,  0.0000e+00,  2.9802e-08],\n",
      "          [-7.4506e-09,  0.0000e+00,  0.0000e+00,  5.9605e-08,  0.0000e+00,\n",
      "           -1.1921e-07, -5.9605e-08,  1.4901e-08]],\n",
      "\n",
      "         [[ 0.0000e+00, -5.9605e-08, -1.1921e-07, -5.9605e-08,  5.9605e-08,\n",
      "            0.0000e+00, -2.9802e-08,  0.0000e+00],\n",
      "          [-7.4506e-09,  0.0000e+00,  0.0000e+00, -1.1921e-07, -5.9605e-08,\n",
      "           -1.1921e-07,  0.0000e+00, -2.9802e-08],\n",
      "          [ 2.9802e-08, -1.4901e-08, -5.9605e-08,  5.9605e-08, -5.9605e-08,\n",
      "           -2.3842e-07, -5.9605e-08,  0.0000e+00],\n",
      "          [ 1.4901e-08, -2.9802e-08,  5.9605e-08,  0.0000e+00, -5.9605e-08,\n",
      "            0.0000e+00,  0.0000e+00, -2.9802e-08],\n",
      "          [-2.9802e-08, -1.4901e-08,  3.7253e-08, -7.4506e-08, -5.9605e-08,\n",
      "            0.0000e+00, -5.9605e-08, -2.9802e-08],\n",
      "          [ 2.9802e-08, -1.4901e-08, -5.9605e-08, -5.9605e-08, -5.9605e-08,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00, -2.9802e-08, -5.9605e-08,  0.0000e+00, -1.1921e-07,\n",
      "            0.0000e+00, -2.9802e-08, -2.9802e-08],\n",
      "          [ 0.0000e+00, -2.9802e-08,  0.0000e+00, -2.9802e-08, -8.9407e-08,\n",
      "            0.0000e+00, -1.8626e-08,  7.4506e-09]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00, -2.9802e-08,  2.9802e-08,\n",
      "           -2.9802e-08,  5.9605e-08, -1.4901e-08],\n",
      "          [ 0.0000e+00, -2.9802e-08, -4.4703e-08,  3.3528e-08, -1.4901e-08,\n",
      "           -1.4901e-08, -1.4901e-08,  2.9802e-08],\n",
      "          [ 5.9605e-08, -5.9605e-08,  5.9605e-08, -2.9802e-08,  2.9802e-08,\n",
      "           -5.9605e-08,  2.9802e-08,  5.2154e-08],\n",
      "          [ 1.1921e-07, -5.9605e-08,  2.9802e-08,  2.9802e-08,  1.4901e-08,\n",
      "            0.0000e+00,  5.9605e-08,  1.4901e-08],\n",
      "          [-1.1921e-07,  3.3528e-08,  3.7253e-08, -2.2352e-08,  0.0000e+00,\n",
      "            9.3132e-10,  5.9605e-08, -2.9802e-08],\n",
      "          [ 5.9605e-08, -2.9802e-08,  1.4901e-08,  0.0000e+00,  2.9802e-08,\n",
      "            0.0000e+00,  2.9802e-08,  2.9802e-08],\n",
      "          [-4.4703e-08,  2.9802e-08, -2.9802e-08,  5.9605e-08, -5.9605e-08,\n",
      "            8.9407e-08,  2.9802e-08,  0.0000e+00],\n",
      "          [ 0.0000e+00, -2.9802e-08,  2.9802e-08, -6.5193e-08,  2.9802e-08,\n",
      "           -1.4901e-08,  2.2352e-08,  2.9802e-08]]]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "QuantConv2d.N_bits=8\n",
    "inputs =torch.randn(1, 3, 8, 8)\n",
    "inputs = inputs.abs()\n",
    "print(len(inputs[inputs==0]))\n",
    "quant_conv = QuantConv2d(3, 4, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "hook_list = []\n",
    "quant_hook = Quantized_Hook(\"test\",quant_conv)\n",
    "out = quant_conv(inputs)\n",
    "print(quant_hook.quan_output * quant_hook.input_step_size * quant_hook.weight_step_size - out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 8, 8])\n",
      "torch.Size([4, 3, 3, 3])\n",
      "torch.Size([1, 27, 64])\n",
      "torch.Size([4, 3, 3, 3])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_tensor = copy.deepcopy(quant_hook.quan_weight.data)\n",
    "input_tensor = copy.deepcopy(quant_hook.quan_input.data)\n",
    "print(input_tensor.shape)\n",
    "print(weight_tensor.shape)\n",
    "fold_param = dict(kernel_size=quant_hook.module.kernel_size, dilation=quant_hook.module.dilation, padding=quant_hook.module.padding, stride=quant_hook.module.stride)\n",
    "unfold_module = nn.Unfold(**fold_param)\n",
    "unfold_out = unfold_module(input_tensor)\n",
    "print(unfold_out.shape)\n",
    "weight_2d = weight_tensor.reshape(weight_tensor.shape[0], -1)\n",
    "print(weight_tensor.shape)\n",
    "out = F.conv2d(input_tensor, weight_tensor, stride=fold_param['stride'], padding=fold_param['padding'])\n",
    "print(out - F.linear(unfold_out.transpose(1, 2), weight_2d, bias=None).transpose(1,2).reshape(out.shape))\n",
    "F.linear(unfold_out.transpose(1, 2), weight_2d, bias=None).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input matrix shape :  torch.Size([1, 27, 64])\n",
      "weight matrix shape :  torch.Size([4, 3, 3, 3])\n",
      "weight matrix shape :  torch.Size([4, 27])\n",
      "matmul_result shape :  torch.Size([1, 64, 4])\n",
      "re generate conv2d shape :  torch.Size([1, 4, 8, 8])\n",
      "compare conv2d and im2col dot product result :  tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "### Y = W*X ==> Y = (X * W).transpose(1, 2)\n",
    "\n",
    "print(\"input matrix shape : \", unfold_out.shape) # (batch size, kernel_size * kernel_size * in_channels, output_size * output_size) \n",
    "unfold_input = unfold_out.transpose(1, 2) # (batch size, output_size * output_size, kernel_size * kernel_size * in_channels)\n",
    "print(\"weight matrix shape : \", weight_tensor.shape) # (out_channels, kernel_size * kernel_size * in_channels)\n",
    "\n",
    "weight_2d = weight_tensor.reshape(weight_tensor.shape[0], -1) \n",
    "print(\"weight matrix shape : \", weight_2d.shape) # (out_channels, kernel_size * kernel_size * in_channels)\n",
    "weight_2d_dot = weight_2d.transpose(0,1) # (kernel_size * kernel_size * in_channels, out_channels)\n",
    "# dot product (input matrix, weight matrix)\n",
    "matmul_result = torch.matmul(unfold_input, weight_2d_dot)\n",
    "print(\"matmul_result shape : \", matmul_result.shape) # (batch size, output_size * output_size, out_channels)\n",
    "print(\"re generate conv2d shape : \", matmul_result.transpose(1,2).reshape(out.shape).shape) # (batch size, out_channels, output_size, output_size)\n",
    "print(\"compare conv2d and im2col dot product result : \",out - matmul_result.transpose(1,2).reshape(out.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.8645, -2.7106, -1.7416, 21.1250, -2.9348,  1.5684, -2.6374, -2.5502,\n",
       "         -4.6618, -2.6044]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QuantConv2d.N_bits = 8\n",
    "c_model = change_model(model)\n",
    "for data, label in dataloader['test']:\n",
    "    break\n",
    "\n",
    "inputs = data[:1].cuda()\n",
    "\n",
    "hook_list = []\n",
    "\n",
    "for name, module in c_model.named_modules():\n",
    "    if isinstance(module, QuantConv2d):\n",
    "        hook_list.append(Quantized_Hook(name, module))\n",
    " \n",
    "c_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1\n",
      "input shape torch.Size([1, 3, 32, 32]), weight shape torch.Size([16, 3, 3, 3])\n",
      "input 2d shape : torch.Size([1024, 27])\n",
      "weight 2d shape : torch.Size([27, 16])\n",
      "matmul out shape : torch.Size([1024, 16])\n",
      "layer1.0.conv1\n",
      "input shape torch.Size([1, 16, 32, 32]), weight shape torch.Size([16, 16, 3, 3])\n",
      "input 2d shape : torch.Size([1024, 144])\n",
      "weight 2d shape : torch.Size([144, 16])\n",
      "matmul out shape : torch.Size([1024, 16])\n",
      "layer1.0.conv2\n",
      "input shape torch.Size([1, 16, 32, 32]), weight shape torch.Size([16, 16, 3, 3])\n",
      "input 2d shape : torch.Size([1024, 144])\n",
      "weight 2d shape : torch.Size([144, 16])\n",
      "matmul out shape : torch.Size([1024, 16])\n",
      "layer1.1.conv1\n",
      "input shape torch.Size([1, 16, 32, 32]), weight shape torch.Size([16, 16, 3, 3])\n",
      "input 2d shape : torch.Size([1024, 144])\n",
      "weight 2d shape : torch.Size([144, 16])\n",
      "matmul out shape : torch.Size([1024, 16])\n",
      "layer1.1.conv2\n",
      "input shape torch.Size([1, 16, 32, 32]), weight shape torch.Size([16, 16, 3, 3])\n",
      "input 2d shape : torch.Size([1024, 144])\n",
      "weight 2d shape : torch.Size([144, 16])\n",
      "matmul out shape : torch.Size([1024, 16])\n",
      "layer1.2.conv1\n",
      "input shape torch.Size([1, 16, 32, 32]), weight shape torch.Size([16, 16, 3, 3])\n",
      "input 2d shape : torch.Size([1024, 144])\n",
      "weight 2d shape : torch.Size([144, 16])\n",
      "matmul out shape : torch.Size([1024, 16])\n",
      "layer1.2.conv2\n",
      "input shape torch.Size([1, 16, 32, 32]), weight shape torch.Size([16, 16, 3, 3])\n",
      "input 2d shape : torch.Size([1024, 144])\n",
      "weight 2d shape : torch.Size([144, 16])\n",
      "matmul out shape : torch.Size([1024, 16])\n",
      "layer2.0.conv1\n",
      "input shape torch.Size([1, 16, 32, 32]), weight shape torch.Size([32, 16, 3, 3])\n",
      "input 2d shape : torch.Size([256, 144])\n",
      "weight 2d shape : torch.Size([144, 32])\n",
      "matmul out shape : torch.Size([256, 32])\n",
      "layer2.0.conv2\n",
      "input shape torch.Size([1, 32, 16, 16]), weight shape torch.Size([32, 32, 3, 3])\n",
      "input 2d shape : torch.Size([256, 288])\n",
      "weight 2d shape : torch.Size([288, 32])\n",
      "matmul out shape : torch.Size([256, 32])\n",
      "layer2.1.conv1\n",
      "input shape torch.Size([1, 32, 16, 16]), weight shape torch.Size([32, 32, 3, 3])\n",
      "input 2d shape : torch.Size([256, 288])\n",
      "weight 2d shape : torch.Size([288, 32])\n",
      "matmul out shape : torch.Size([256, 32])\n",
      "layer2.1.conv2\n",
      "input shape torch.Size([1, 32, 16, 16]), weight shape torch.Size([32, 32, 3, 3])\n",
      "input 2d shape : torch.Size([256, 288])\n",
      "weight 2d shape : torch.Size([288, 32])\n",
      "matmul out shape : torch.Size([256, 32])\n",
      "layer2.2.conv1\n",
      "input shape torch.Size([1, 32, 16, 16]), weight shape torch.Size([32, 32, 3, 3])\n",
      "input 2d shape : torch.Size([256, 288])\n",
      "weight 2d shape : torch.Size([288, 32])\n",
      "matmul out shape : torch.Size([256, 32])\n",
      "layer2.2.conv2\n",
      "input shape torch.Size([1, 32, 16, 16]), weight shape torch.Size([32, 32, 3, 3])\n",
      "input 2d shape : torch.Size([256, 288])\n",
      "weight 2d shape : torch.Size([288, 32])\n",
      "matmul out shape : torch.Size([256, 32])\n",
      "layer3.0.conv1\n",
      "input shape torch.Size([1, 32, 16, 16]), weight shape torch.Size([64, 32, 3, 3])\n",
      "input 2d shape : torch.Size([64, 288])\n",
      "weight 2d shape : torch.Size([288, 64])\n",
      "matmul out shape : torch.Size([64, 64])\n",
      "layer3.0.conv2\n",
      "input shape torch.Size([1, 64, 8, 8]), weight shape torch.Size([64, 64, 3, 3])\n",
      "input 2d shape : torch.Size([64, 576])\n",
      "weight 2d shape : torch.Size([576, 64])\n",
      "matmul out shape : torch.Size([64, 64])\n",
      "layer3.1.conv1\n",
      "input shape torch.Size([1, 64, 8, 8]), weight shape torch.Size([64, 64, 3, 3])\n",
      "input 2d shape : torch.Size([64, 576])\n",
      "weight 2d shape : torch.Size([576, 64])\n",
      "matmul out shape : torch.Size([64, 64])\n",
      "layer3.1.conv2\n",
      "input shape torch.Size([1, 64, 8, 8]), weight shape torch.Size([64, 64, 3, 3])\n",
      "input 2d shape : torch.Size([64, 576])\n",
      "weight 2d shape : torch.Size([576, 64])\n",
      "matmul out shape : torch.Size([64, 64])\n",
      "layer3.2.conv1\n",
      "input shape torch.Size([1, 64, 8, 8]), weight shape torch.Size([64, 64, 3, 3])\n",
      "input 2d shape : torch.Size([64, 576])\n",
      "weight 2d shape : torch.Size([576, 64])\n",
      "matmul out shape : torch.Size([64, 64])\n",
      "layer3.2.conv2\n",
      "input shape torch.Size([1, 64, 8, 8]), weight shape torch.Size([64, 64, 3, 3])\n",
      "input 2d shape : torch.Size([64, 576])\n",
      "weight 2d shape : torch.Size([576, 64])\n",
      "matmul out shape : torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "for hook in hook_list:\n",
    "    print(hook.name)\n",
    "    print(f\"input shape {hook.input[0].shape}, weight shape {hook.weight.shape}\")\n",
    "    wq = hook.quan_weight.data.view(hook.quan_weight.shape[0], -1).transpose(0, 1) #  output_channels, in_channels, kernel_size, kernel_size) => (kernel_size * kernel_size * in_c, output_channels)\n",
    "    wq = wq + 128 # -128 ~ 127 to 0 ~ 255\n",
    "    wq = wq.type(torch.int32).to(device='cpu')\n",
    "    inq = hook.quan_input.data\n",
    "    unfold_param = dict(kernel_size=hook.module.kernel_size, dilation=hook.module.dilation, padding=hook.module.padding, stride=hook.module.stride)\n",
    "    unfold_inq = nn.Unfold(**unfold_param)(inq)\n",
    "    unfold_inq = unfold_inq.transpose(1, 2).squeeze(0) # (batch_size (=1), kernel_size * kernel_size * in_channels, output_size * output_size) => (output_size * output_size, kernel_size * kernel_size * in_channels)\n",
    "    unfold_inq = unfold_inq.type(torch.int32).to(device='cpu')\n",
    "    matmul_out = torch.matmul(unfold_inq, wq).to(torch.int32) # (output_size * output_size, kernel_size * kernel_size * in_channels) * (kernel_size * kernel_size * in_channels, output_channels) => (output_size * output_size, output_channels) \n",
    "    print(f\"input 2d shape :\", unfold_inq.shape)\n",
    "    print(f\"weight 2d shape :\", wq.shape)\n",
    "    print(f\"matmul out shape :\", matmul_out.shape)\n",
    "    unfold_inq_np = unfold_inq.numpy()\n",
    "    wq_np = wq.numpy()\n",
    "    matmul_out_np = matmul_out.numpy()\n",
    "    \n",
    "    np.savetxt(f\"/app/tensor_result/{hook.name}_input.txt\", unfold_inq_np, fmt='%d')\n",
    "    np.savetxt(f\"/app/tensor_result/{hook.name}_weight.txt\", wq_np, fmt='%d')\n",
    "    np.savetxt(f\"/app/tensor_result/{hook.name}_matmul_out.txt\", matmul_out_np, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(996891, dtype=torch.int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matmul_out.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([100, 200], dtype=torch.uint8)\n",
      "tensor([16, 64], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "g = torch.IntTensor([100, 200])\n",
    "g = g.type(torch.uint8)\n",
    "print(g)\n",
    "print(g*g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
